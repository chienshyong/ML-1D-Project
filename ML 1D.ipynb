{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#Read the data\n",
    "language = 'RU'\n",
    "file_path = f\"Data/{language}/train\"\n",
    "labeled_data = []\n",
    "\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "labeled_data.append(('START', 'START'))\n",
    "for line in lines:\n",
    "    line = line.strip()\n",
    "    if line:\n",
    "        try:\n",
    "            data_point, label = line.rsplit(' ', 1) #Split by only the last space in a line, cus of some noise in RU data\n",
    "            labeled_data.append((label, data_point))  #Storing as a list of tuples (label, data_point)\n",
    "        except:\n",
    "            print(\"Error at line \" + line)\n",
    "    else:\n",
    "        labeled_data.append(('STOP', 'STOP')) #If line is empty it signals the end of a sentence, and the start of a new one\n",
    "        labeled_data.append(('START', 'START'))\n",
    "labeled_data.append(('STOP', 'STOP'))\n",
    "\n",
    "#Global data\n",
    "df = pd.DataFrame(labeled_data, columns=['Label', 'Data'])\n",
    "labels = df['Label'].unique()\n",
    "words = df['Data'].unique()\n",
    "label_counts = df['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label  B-negative  B-neutral  B-positive  I-negative  I-neutral  I-positive  \\\n",
      "!        0.000010   0.000010    0.000010    0.000010   0.000010    0.000010   \n",
      "\"        0.000010   0.009662    0.002693    0.014184   0.058824    0.045226   \n",
      "%        0.000010   0.000010    0.000010    0.000010   0.000010    0.000010   \n",
      "&        0.000010   0.000010    0.000010    0.000010   0.000010    0.003350   \n",
      "'        0.000010   0.000010    0.000010    0.000010   0.000010    0.000010   \n",
      "...           ...        ...         ...         ...        ...         ...   \n",
      "–        0.000010   0.000010    0.000010    0.000010   0.000010    0.000010   \n",
      "—        0.000010   0.000010    0.000010    0.000010   0.000010    0.000010   \n",
      "…        0.000010   0.000010    0.000010    0.000010   0.000010    0.000010   \n",
      "№        0.000010   0.000010    0.000010    0.000010   0.000010    0.001675   \n",
      "#UNK#    0.002252   0.004831    0.000539    0.007092   0.014706    0.001675   \n",
      "\n",
      "Label         O  START  STOP  \n",
      "!      0.021912    0.0   0.0  \n",
      "\"      0.004590    0.0   0.0  \n",
      "%      0.000296    0.0   0.0  \n",
      "&      0.000025    0.0   0.0  \n",
      "'      0.000049    0.0   0.0  \n",
      "...         ...    ...   ...  \n",
      "–      0.000247    0.0   0.0  \n",
      "—      0.000025    0.0   0.0  \n",
      "…      0.000123    0.0   0.0  \n",
      "№      0.000025    0.0   0.0  \n",
      "#UNK#  0.000025    0.0   0.0  \n",
      "\n",
      "[7883 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "#Part 1\n",
    "#Generate Emission Matrix\n",
    "#emission_matrix[label][word]\n",
    "\n",
    "word_label_counts_matrix = pd.crosstab(df['Data'], df['Label'])\n",
    "k = 1\n",
    "unk_row = pd.DataFrame([[k]*(len(word_label_counts_matrix.columns)-2)+[0]*2], columns=word_label_counts_matrix.columns, index=['#UNK#']) # Create a new row with index '#UNK#' and values all set to k\n",
    "word_label_counts_matrix = pd.concat([word_label_counts_matrix, unk_row]) # Append the new row to the DataFrame\n",
    "emission_matrix = word_label_counts_matrix.div(label_counts, axis=1)\n",
    "\n",
    "# Replace any value of 0 with 0.00001. Necessary otherwise some sentences will have no valid path.\n",
    "def replace_zero_except_start_stop(val):\n",
    "    if val == 0 and col not in ['START', 'STOP']:\n",
    "        return 0.00001\n",
    "    return val\n",
    "\n",
    "for col in emission_matrix.columns:\n",
    "    emission_matrix[col] = emission_matrix[col].apply(replace_zero_except_start_stop)\n",
    "\n",
    "# Find the highest value and its corresponding column name for each row\n",
    "max_value_columns = emission_matrix.idxmax(axis=1)\n",
    "max_emission = pd.DataFrame({'Max_Emission': max_value_columns})\n",
    "\n",
    "print(emission_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 1\n",
    "#Read test data and implement naive max emission estimation\n",
    "\n",
    "train_file_path = f'Data/{language}/train'\n",
    "dev_file_path = f'Data/{language}/dev.in'\n",
    "dev_out_file_path = f'Outputs/{language}/dev.p1.out'\n",
    "\n",
    "def calc_argmax(word):\n",
    "    \"\"\"calculates the maximum probability\"\"\"\n",
    "    if (df['Data'] == word).sum() == 0:\n",
    "        word = '#UNK#'\n",
    "    em_prob = {}\n",
    "    for label in labels:\n",
    "        em_prob[label] = emission_matrix[label][word]\n",
    "    return max(em_prob, key=em_prob.get)\n",
    "        \n",
    "outputs = []\n",
    "\n",
    "with open(dev_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    open(dev_out_file_path, \"w\")\n",
    "    for line in f:\n",
    "        line = line.replace(\"\\n\", \"\")\n",
    "        if line != \"\":\n",
    "            tag = calc_argmax(line)\n",
    "        else:\n",
    "            tag = \"\"\n",
    "        output = f\"{line} {tag}\"\n",
    "        outputs.append(output)\n",
    "\n",
    "# Open the file in write mode and write each element of the array as a new line\n",
    "with open(dev_out_file_path, \"w\", encoding='utf-8') as file:\n",
    "    for output in outputs:\n",
    "        file.write(output + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               START B-positive         O I-positive      STOP B-negative  \\\n",
      "START            0.0   0.112308  0.844597        0.0  0.000326   0.021548   \n",
      "B-positive       0.0   0.001616   0.80937   0.188476  0.000539        0.0   \n",
      "O                0.0   0.037235  0.874451        0.0  0.075482   0.009327   \n",
      "I-positive       0.0   0.001675  0.582915   0.413735  0.001675        0.0   \n",
      "STOP        0.999674        0.0       0.0        0.0       0.0        0.0   \n",
      "B-negative       0.0        0.0  0.815315        0.0       0.0        0.0   \n",
      "I-negative       0.0        0.0  0.574468        0.0  0.007092        0.0   \n",
      "B-neutral        0.0        0.0  0.864734        0.0       0.0        0.0   \n",
      "I-neutral        0.0        0.0  0.411765        0.0       0.0        0.0   \n",
      "\n",
      "           I-negative B-neutral I-neutral  \n",
      "START             0.0  0.021221       0.0  \n",
      "B-positive        0.0       0.0       0.0  \n",
      "O                 0.0  0.003504       0.0  \n",
      "I-positive        0.0       0.0       0.0  \n",
      "STOP              0.0       0.0       0.0  \n",
      "B-negative   0.184685       0.0       0.0  \n",
      "I-negative    0.41844       0.0       0.0  \n",
      "B-neutral         0.0       0.0  0.135266  \n",
      "I-neutral         0.0       0.0  0.588235  \n"
     ]
    }
   ],
   "source": [
    "#Part 2\n",
    "#Generate MLE transition matrix\n",
    "#transition_matrix[label][prev_label] -> likelihood of label fallowing prev_label\n",
    "\n",
    "def max_likelihood_estimation(prev_label, label):\n",
    "    prev_label_count = (df['Label'] == prev_label).sum()\n",
    "    df['Previous_Label'] = df['Label'].shift(1)\n",
    "    condition = (df['Label'] == label) & (df['Previous_Label'] == prev_label)\n",
    "    label_follows_previous_count = condition.sum()\n",
    "    return label_follows_previous_count/prev_label_count\n",
    "\n",
    "transition_matrix = pd.DataFrame(index=labels, columns=labels)\n",
    "for prev_label in labels:\n",
    "    for label in labels:\n",
    "        transition_matrix[label][prev_label] = max_likelihood_estimation(prev_label, label)\n",
    "\n",
    "print(transition_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 2\n",
    "#Viterbi Algorithm\n",
    "\n",
    "def viterbi(sentence): #sentence eg ['START','Plato','degustación',':','un','poco','abundante','de','más',',','pero','bien','cocinado','.','STOP']\n",
    "    viterbi_matrix = pd.DataFrame(index=labels, columns=sentence)\n",
    "    best_back_path_matrix = pd.DataFrame(index=labels, columns=sentence)\n",
    "\n",
    "    #Init START emitting START to 1\n",
    "    #viterbi_matrix[label][word] represents the highest score of path from START to this node\n",
    "    viterbi_matrix.loc[:, 'START'] = 0\n",
    "    viterbi_matrix.loc['START', 'START'] = 1\n",
    "    best_back_path_matrix.loc[:, 'START'] = None\n",
    "    best_back_path_matrix.iloc[:, 1] = 'START'\n",
    "\n",
    "    #DP algorithm\n",
    "    for word_index in range(1,len(sentence)): #Start from second element, skip START\n",
    "        for label in labels:\n",
    "            max_score = 0\n",
    "            for prev_label in labels:\n",
    "                score = viterbi_matrix.iloc[viterbi_matrix.index.get_loc(prev_label), word_index-1] * emission_matrix[label][sentence[word_index]] * transition_matrix[label][prev_label]\n",
    "                if score > max_score:\n",
    "                    max_score = score\n",
    "                    best_back_path_matrix.iloc[viterbi_matrix.index.get_loc(label), word_index] = prev_label\n",
    "            viterbi_matrix.iloc[viterbi_matrix.index.get_loc(label), word_index] = max_score\n",
    "            if max_score == 0: #No chance for this label, ignore possible path\n",
    "                best_back_path_matrix.iloc[best_back_path_matrix.index.get_loc(label), word_index] = None\n",
    "\n",
    "    #From highest scoring value at STOP, find the best back path.\n",
    "    pointer = 'STOP'\n",
    "    best_path = []\n",
    "    for word_index in range(len(sentence)-1,-1,-1):\n",
    "        best_path.append(pointer)\n",
    "        pointer = best_back_path_matrix.iloc[best_back_path_matrix.index.get_loc(pointer), word_index]\n",
    "    best_path = best_path[::-1] #Reverse the array\n",
    "    best_path = best_path[1:-1] #Remove START and STOP\n",
    "    \n",
    "    return best_path\n",
    "\n",
    "#Helper function to replace all new words with #UNK# and add START and STOP\n",
    "def add_startstop_replace_new(sentence):\n",
    "    for index, word in enumerate(sentence):\n",
    "        if (df['Data'] == word).sum() == 0:\n",
    "            sentence[index] = '#UNK#'\n",
    "    sentence.insert(0, 'START')\n",
    "    sentence.append('STOP')\n",
    "\n",
    "#Wrapper function, call this.\n",
    "def predict_sentence_sentiment(sentence):\n",
    "    sentence_copy = sentence.copy()\n",
    "    add_startstop_replace_new(sentence_copy)\n",
    "    return viterbi(sentence_copy)\n",
    "\n",
    "#sentence = ['Plato','degustación',':','un','poco','abundante','de','más',',','pero','bien','cocinado','.']\n",
    "#predict_sentence_sentiment(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 2\n",
    "#Predict sentiment of dev.in, write to dev.p2.out\n",
    "\n",
    "#Read the data\n",
    "file_path = f\"Data/{language}/dev.in\"\n",
    "\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "#Read data into array of sentence arrays\n",
    "sentences = [[]]\n",
    "sentence_number = 0\n",
    "for line in lines:\n",
    "    line = line.strip()\n",
    "    if line:\n",
    "        sentences[sentence_number].append(line)\n",
    "    else:\n",
    "        sentences.append([])\n",
    "        sentence_number += 1\n",
    "\n",
    "sentences_with_prediction = []\n",
    "for sentence in sentences:\n",
    "    try:\n",
    "        prediction = predict_sentence_sentiment(sentence)\n",
    "        # Using list comprehension\n",
    "        sentence_with_prediction = [s1 + ' ' + s2 for s1, s2 in zip(sentence, prediction)]\n",
    "        sentences_with_prediction.append(sentence_with_prediction)\n",
    "    except:\n",
    "        print(\"Error at sentence: \")\n",
    "        print(sentence)\n",
    "\n",
    "# Name of the output file\n",
    "file_name = f\"Outputs/{language}/dev.p2.out\"\n",
    "\n",
    "# Open the file in write mode and write each element of the array as a new line\n",
    "with open(file_name, \"w\", encoding='utf-8') as file:\n",
    "    for sentence in sentences_with_prediction:\n",
    "        for line in sentence:\n",
    "            file.write(line + \"\\n\")\n",
    "        file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 3\n",
    "#Viterbi Algorithm modified - find k-th best path\n",
    "\n",
    "def viterbi_multiple(sentence, k): #sentence eg ['START','Plato','degustación',':','un','poco','abundante','de','más',',','pero','bien','cocinado','.','STOP']\n",
    "    viterbi_matrix = np.zeros((len(labels), len(sentence), k)) \n",
    "    best_back_path_matrix = np.empty((len(labels), len(sentence), k), dtype=object) #best_bath_path_matrix[0,1,2] = (2,1) means third best score to first label, second word points back to second label, second best score\n",
    "\n",
    "    START_LABEL_INDEX = np.where(labels == 'START')[0][0]\n",
    "    STOP_LABEL_INDEX = np.where(labels == 'STOP')[0][0]\n",
    "\n",
    "    #Init START emitting START to 1\n",
    "    #viterbi_matrix[0,1,2] = 0.001 means third best score to first label, second word\n",
    "    viterbi_matrix[START_LABEL_INDEX, 0, 0] = 1 #Init (START, START, path 0) to 1\n",
    "    best_back_path_matrix[:, :, :] = None\n",
    "\n",
    "    #DP algorithm\n",
    "    for word_index in range(1,len(sentence)): #Start from second element, skip START\n",
    "        for label_index, label in enumerate(labels):\n",
    "            for prev_label_index, prev_label in enumerate(labels):\n",
    "                for depth in range(k): #Iterate over all k best paths from previous node\n",
    "                    score = viterbi_matrix[prev_label_index, word_index-1, depth] * emission_matrix[label][sentence[word_index]] * transition_matrix[label][prev_label]\n",
    "   \n",
    "                    #Find the position to insert the new value to maintain descending order\n",
    "                    #Use negation as searchsorted only works for ascending\n",
    "                    position = np.searchsorted(-viterbi_matrix[label_index, word_index, :], -score, side='right')\n",
    "\n",
    "                    #If position == k, the score is smaller than existing scores\n",
    "                    #Shift all values in both matrices from 'position' by one, then insert new element\n",
    "                    if position < k:\n",
    "                        for pointer in range(k-1, position, -1):\n",
    "                            viterbi_matrix[label_index, word_index, pointer] = viterbi_matrix[label_index, word_index, pointer-1]\n",
    "                            best_back_path_matrix[label_index, word_index, pointer] = best_back_path_matrix[label_index, word_index, pointer-1]\n",
    "                        viterbi_matrix[label_index, word_index, position] = score\n",
    "                        best_back_path_matrix[label_index, word_index, position] = (prev_label_index, depth)\n",
    "\n",
    "    final_scores = viterbi_matrix[STOP_LABEL_INDEX, len(sentence)-1, :]\n",
    "\n",
    "    #From all the highest scoring paths at STOP, find the best back path.\n",
    "    best_paths = []\n",
    "    best_path = []\n",
    "    for path_number in range(k):\n",
    "        if final_scores[path_number] != 0: #If score is 0 (no path), just put as previous path.\n",
    "            pointer = STOP_LABEL_INDEX\n",
    "            depth_pointer = path_number\n",
    "            best_path = []\n",
    "            for word_index in range(len(sentence)-1,-1,-1):\n",
    "                best_path.append(labels[pointer])\n",
    "                if word_index > 0:\n",
    "                    back_path = best_back_path_matrix[pointer, word_index, depth_pointer]\n",
    "                    pointer = back_path[0]\n",
    "                    depth_pointer = back_path[1]\n",
    "            best_path = best_path[::-1] #Reverse the array\n",
    "            best_path = best_path[1:-1] #Remove START and STOP\n",
    "            best_paths.append(best_path)\n",
    "        else:\n",
    "            best_paths.append(best_path)\n",
    "    return best_paths\n",
    "\n",
    "#Wrapper function, call this.\n",
    "def predict_sentence_sentiment_multiple(sentence, k):\n",
    "    sentence_copy = sentence.copy()\n",
    "    add_startstop_replace_new(sentence_copy) #Same as qn2\n",
    "    return viterbi_multiple(sentence_copy, k)\n",
    "\n",
    "#sentence = ['Plato','degustación',':','un','poco','abundante','de','más',',','pero','bien','cocinado','.']\n",
    "#predict_sentence_sentiment_multiple(sentence, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 3\n",
    "#Predict sentiment of dev.in, output to dev.p3.2nd.out and dev.p3.8th.out\n",
    "\n",
    "#Read the data\n",
    "file_path = f\"Data/{language}/dev.in\"\n",
    "\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "#Read data into array of sentence arrays\n",
    "sentences = [[]]\n",
    "sentence_number = 0\n",
    "for line in lines:\n",
    "    line = line.strip()\n",
    "    if line:\n",
    "        sentences[sentence_number].append(line)\n",
    "    else:\n",
    "        sentences.append([])\n",
    "        sentence_number += 1\n",
    "\n",
    "second_sentences_with_prediction = []\n",
    "eighth_sentences_with_prediction = []\n",
    "for sentence in sentences:\n",
    "    try:\n",
    "        prediction = predict_sentence_sentiment_multiple(sentence, 8)\n",
    "        second_sentence_with_prediction = [s1 + ' ' + s2 for s1, s2 in zip(sentence, prediction[1])]\n",
    "        second_sentences_with_prediction.append(second_sentence_with_prediction)\n",
    "        eighth_sentence_with_prediction = [s1 + ' ' + s2 for s1, s2 in zip(sentence, prediction[7])]\n",
    "        eighth_sentences_with_prediction.append(eighth_sentence_with_prediction)\n",
    "    except:\n",
    "        print(\"Error at sentence: \")\n",
    "        print(sentence)\n",
    "\n",
    "file_name = f\"Outputs/{language}/dev.p3.2nd.out\"\n",
    "with open(file_name, \"w\", encoding='utf-8') as file:\n",
    "    for sentence in second_sentences_with_prediction:\n",
    "        for line in sentence:\n",
    "            file.write(line + \"\\n\")\n",
    "        file.write(\"\\n\")\n",
    "\n",
    "file_name = f\"Outputs/{language}/dev.p3.8th.out\"\n",
    "with open(file_name, \"w\", encoding='utf-8') as file:\n",
    "    for sentence in eighth_sentences_with_prediction:\n",
    "        for line in sentence:\n",
    "            file.write(line + \"\\n\")\n",
    "        file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 4\n",
    "#Design Challenge\n",
    "#Introduce 'entity penalty': a normalization hyperparameter that penalizes the score based on the number of entities.\n",
    "#The previous algorithm tend to generate too many entities: twice as many as gold entities.\n",
    "#Higher entity penalty -> less entities predicted\n",
    "#Test different entity penalties to find best hyperparameter\n",
    "\n",
    "def viterbi_multiple(sentence, k): #sentence eg ['START','Plato','degustación',':','un','poco','abundante','de','más',',','pero','bien','cocinado','.','STOP']\n",
    "    viterbi_matrix = np.zeros((len(labels), len(sentence), k)) \n",
    "    best_back_path_matrix = np.empty((len(labels), len(sentence), k), dtype=object) #best_bath_path_matrix[0,1,2] = (2,1) means third best score to first label, second word points back to second label, second best score\n",
    "\n",
    "    START_LABEL_INDEX = np.where(labels == 'START')[0][0]\n",
    "    STOP_LABEL_INDEX = np.where(labels == 'STOP')[0][0]\n",
    "\n",
    "    #Init START emitting START to 1\n",
    "    #viterbi_matrix[0,1,2] = 0.001 means third best score to first label, second word\n",
    "    viterbi_matrix[START_LABEL_INDEX, 0, 0] = 1 #Init (START, START, path 0) to 1\n",
    "    best_back_path_matrix[:, :, :] = None\n",
    "\n",
    "    #DP algorithm\n",
    "    for word_index in range(1,len(sentence)): #Start from second element, skip START\n",
    "        for label_index, label in enumerate(labels):\n",
    "            for prev_label_index, prev_label in enumerate(labels):\n",
    "                for depth in range(k): #Iterate over all k best paths from previous node\n",
    "                    score = viterbi_matrix[prev_label_index, word_index-1, depth] * emission_matrix[label][sentence[word_index]] * transition_matrix[label][prev_label]\n",
    "   \n",
    "                    #Find the position to insert the new value to maintain descending order\n",
    "                    #Use negation as searchsorted only works for ascending\n",
    "                    position = np.searchsorted(-viterbi_matrix[label_index, word_index, :], -score, side='right')\n",
    "\n",
    "                    #If position == k, the score is smaller than existing scores\n",
    "                    #Shift all values in both matrices from 'position' by one, then insert new element\n",
    "                    if position < k:\n",
    "                        for pointer in range(k-1, position, -1):\n",
    "                            viterbi_matrix[label_index, word_index, pointer] = viterbi_matrix[label_index, word_index, pointer-1]\n",
    "                            best_back_path_matrix[label_index, word_index, pointer] = best_back_path_matrix[label_index, word_index, pointer-1]\n",
    "                        viterbi_matrix[label_index, word_index, position] = score\n",
    "                        best_back_path_matrix[label_index, word_index, position] = (prev_label_index, depth)\n",
    "\n",
    "    final_scores = viterbi_matrix[STOP_LABEL_INDEX, len(sentence)-1, :]\n",
    "\n",
    "    #From all the highest scoring paths at STOP, find the best back path.\n",
    "    best_paths = []\n",
    "    best_path = []\n",
    "    for path_number in range(k):\n",
    "        if final_scores[path_number] != 0: #If score is 0 (no path), just put as previous path.\n",
    "            pointer = STOP_LABEL_INDEX\n",
    "            depth_pointer = path_number\n",
    "            best_path = []\n",
    "            for word_index in range(len(sentence)-1,-1,-1):\n",
    "                best_path.append(labels[pointer])\n",
    "                if word_index > 0:\n",
    "                    back_path = best_back_path_matrix[pointer, word_index, depth_pointer]\n",
    "                    pointer = back_path[0]\n",
    "                    depth_pointer = back_path[1]\n",
    "            best_path = best_path[::-1] #Reverse the array\n",
    "            best_path = best_path[1:-1] #Remove START and STOP\n",
    "            best_paths.append(best_path)\n",
    "        else:\n",
    "            best_paths.append(best_path)\n",
    "    return best_paths, final_scores\n",
    "\n",
    "#Wrapper function, call this.\n",
    "def predict_sentence_sentiment_entity_penalty(sentence, k, entity_penalty):\n",
    "    sentence_copy = sentence.copy()\n",
    "    add_startstop_replace_new(sentence_copy) #Same as qn2\n",
    "    best_paths, final_scores = viterbi_multiple(sentence_copy, k)\n",
    "\n",
    "    highest_penalized_score = -1\n",
    "    best_penalized_path = None\n",
    "    for path_index, path in enumerate(best_paths):\n",
    "        entity_count = 0\n",
    "        for label in path:\n",
    "            if label == 'B-positive' or label == 'B-negative' or label == 'B_neutral':\n",
    "                entity_count += 1\n",
    "        penalized_score = final_scores[path_index] * (1 - entity_count * entity_penalty)\n",
    "        if penalized_score > highest_penalized_score:\n",
    "            best_penalized_path = path\n",
    "            highest_penalized_score = penalized_score\n",
    "            \n",
    "    return best_penalized_path\n",
    "\n",
    "#sentence = ['Bonito', 'restaurante', ',', 'buen', 'ambiente', 'y', 'servicio', '.']\n",
    "#print(predict_sentence_sentiment_entity_penalty(sentence, 4, 0.99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 4\n",
    "#Predict sentiment of dev.in, output to dev.p4.out\n",
    "\n",
    "#Read the data\n",
    "file_path = f\"Data/{language}/dev.in\"\n",
    "\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "#Read data into array of sentence arrays\n",
    "sentences = [[]]\n",
    "sentence_number = 0\n",
    "for line in lines:\n",
    "    line = line.strip()\n",
    "    if line:\n",
    "        sentences[sentence_number].append(line)\n",
    "    else:\n",
    "        sentences.append([])\n",
    "        sentence_number += 1\n",
    "\n",
    "entity_penalty = 0.1 #Tried several different values, this works well for ES dataset\n",
    "k = 4\n",
    "\n",
    "sentences_with_prediction = []\n",
    "for sentence in sentences:\n",
    "    try:\n",
    "        prediction = predict_sentence_sentiment_entity_penalty(sentence, k, entity_penalty)\n",
    "        # Using list comprehension\n",
    "        sentence_with_prediction = [s1 + ' ' + s2 for s1, s2 in zip(sentence, prediction)]\n",
    "        sentences_with_prediction.append(sentence_with_prediction)\n",
    "    except:\n",
    "        print(\"Error at sentence: \")\n",
    "        print(sentence)\n",
    "\n",
    "# Name of the output file\n",
    "file_name = f\"Outputs/{language}/dev.p4.out\"\n",
    "\n",
    "# Open the file in write mode and write each element of the array as a new line\n",
    "with open(file_name, \"w\", encoding='utf-8') as file:\n",
    "    for sentence in sentences_with_prediction:\n",
    "        for line in sentence:\n",
    "            file.write(line + \"\\n\")\n",
    "        file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
