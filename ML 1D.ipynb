{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#Read the data\n",
    "language = 'ES'\n",
    "file_path = f\"Data/{language}/train\"\n",
    "labeled_data = []\n",
    "\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "labeled_data.append(('START', 'START'))\n",
    "for line in lines:\n",
    "    line = line.strip()\n",
    "    if line:\n",
    "        try:\n",
    "            data_point, label = line.rsplit(' ', 1) #Split by only the last space in a line, cus of some noise in RU data\n",
    "            labeled_data.append((label, data_point))  #Storing as a list of tuples (label, data_point)\n",
    "        except:\n",
    "            print(\"Error at line \" + line)\n",
    "    else:\n",
    "        labeled_data.append(('STOP', 'STOP')) #If line is empty it signals the end of a sentence, and the start of a new one\n",
    "        labeled_data.append(('START', 'START'))\n",
    "labeled_data.append(('STOP', 'STOP'))\n",
    "\n",
    "#Global data\n",
    "df = pd.DataFrame(labeled_data, columns=['Label', 'Data'])\n",
    "labels = df['Label'].unique()\n",
    "words = df['Data'].unique()\n",
    "label_counts = df['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label  B-negative  B-neutral  B-positive  I-negative  I-neutral  I-positive  \\\n",
      "!        0.000010   0.000010    0.000010    0.000010   0.000010    0.000010   \n",
      "\"        0.000010   0.000010    0.000010    0.011696   0.000010    0.006369   \n",
      "%        0.000010   0.000010    0.000010    0.000010   0.000010    0.000010   \n",
      "(        0.000010   0.000010    0.000010    0.000010   0.000010    0.000010   \n",
      ")        0.000010   0.000010    0.000010    0.000010   0.000010    0.000010   \n",
      "...           ...        ...         ...         ...        ...         ...   \n",
      "”        0.000010   0.000010    0.000010    0.000010   0.000010    0.003185   \n",
      "…        0.002625   0.000010    0.000010    0.000010   0.000010    0.000010   \n",
      "″        0.000010   0.000010    0.000010    0.000010   0.000010    0.000010   \n",
      "€        0.000010   0.000010    0.000010    0.000010   0.000010    0.003185   \n",
      "#UNK#    0.002625   0.013889    0.000862    0.005848   0.023256    0.003185   \n",
      "\n",
      "Label         O  START  STOP  \n",
      "!      0.005442    0.0   0.0  \n",
      "\"      0.001343    0.0   0.0  \n",
      "%      0.000413    0.0   0.0  \n",
      "(      0.004512    0.0   0.0  \n",
      ")      0.004512    0.0   0.0  \n",
      "...         ...    ...   ...  \n",
      "”      0.001171    0.0   0.0  \n",
      "…      0.001550    0.0   0.0  \n",
      "″      0.000034    0.0   0.0  \n",
      "€      0.000895    0.0   0.0  \n",
      "#UNK#  0.000034    0.0   0.0  \n",
      "\n",
      "[5043 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "#Part 1\n",
    "#Generate Emission Matrix\n",
    "#emission_matrix[label][word]\n",
    "\n",
    "word_label_counts_matrix = pd.crosstab(df['Data'], df['Label'])\n",
    "k = 1\n",
    "unk_row = pd.DataFrame([[k]*(len(word_label_counts_matrix.columns)-2)+[0]*2], columns=word_label_counts_matrix.columns, index=['#UNK#']) # Create a new row with index '#UNK#' and values all set to k\n",
    "word_label_counts_matrix = pd.concat([word_label_counts_matrix, unk_row]) # Append the new row to the DataFrame\n",
    "emission_matrix = word_label_counts_matrix.div(label_counts, axis=1)\n",
    "\n",
    "# Replace any value of 0 with 0.00001. Necessary otherwise some sentences will have no valid path.\n",
    "def replace_zero_except_start_stop(val):\n",
    "    if val == 0 and col not in ['START', 'STOP']:\n",
    "        return 0.00001\n",
    "    return val\n",
    "\n",
    "for col in emission_matrix.columns:\n",
    "    emission_matrix[col] = emission_matrix[col].apply(replace_zero_except_start_stop)\n",
    "\n",
    "# Find the highest value and its corresponding column name for each row\n",
    "max_value_columns = emission_matrix.idxmax(axis=1)\n",
    "max_emission = pd.DataFrame({'Max_Emission': max_value_columns})\n",
    "\n",
    "print(emission_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO part 1: read test data and implement naive max emission estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               START         O B-positive      STOP B-negative B-neutral  \\\n",
      "START            0.0  0.928418   0.052207  0.000538   0.013994  0.004844   \n",
      "O                0.0   0.88569   0.036508  0.063441   0.012227  0.002135   \n",
      "B-positive       0.0  0.871552   0.002586  0.008621        0.0  0.000862   \n",
      "STOP        0.999462       0.0        0.0       0.0        0.0       0.0   \n",
      "B-negative       0.0  0.811024        0.0  0.010499        0.0       0.0   \n",
      "B-neutral        0.0  0.791667        0.0       0.0        0.0       0.0   \n",
      "I-neutral        0.0  0.348837        0.0       0.0        0.0       0.0   \n",
      "I-positive       0.0  0.426752        0.0  0.003185        0.0       0.0   \n",
      "I-negative       0.0  0.397661        0.0       0.0        0.0       0.0   \n",
      "\n",
      "           I-neutral I-positive I-negative  \n",
      "START            0.0        0.0        0.0  \n",
      "O                0.0        0.0        0.0  \n",
      "B-positive       0.0   0.116379        0.0  \n",
      "STOP             0.0        0.0        0.0  \n",
      "B-negative       0.0        0.0   0.178478  \n",
      "B-neutral   0.208333        0.0        0.0  \n",
      "I-neutral   0.651163        0.0        0.0  \n",
      "I-positive       0.0   0.570064        0.0  \n",
      "I-negative       0.0        0.0   0.602339  \n"
     ]
    }
   ],
   "source": [
    "#Part 2\n",
    "#Generate MLE transition matrix\n",
    "#transition_matrix[label][prev_label] -> likelihood of label fallowing prev_label\n",
    "\n",
    "def max_likelihood_estimation(prev_label, label):\n",
    "    prev_label_count = (df['Label'] == prev_label).sum()\n",
    "    df['Previous_Label'] = df['Label'].shift(1)\n",
    "    condition = (df['Label'] == label) & (df['Previous_Label'] == prev_label)\n",
    "    label_follows_previous_count = condition.sum()\n",
    "    return label_follows_previous_count/prev_label_count\n",
    "\n",
    "transition_matrix = pd.DataFrame(index=labels, columns=labels)\n",
    "for prev_label in labels:\n",
    "    for label in labels:\n",
    "        transition_matrix[label][prev_label] = max_likelihood_estimation(prev_label, label)\n",
    "\n",
    "print(transition_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 2\n",
    "#Viterbi Algorithm\n",
    "\n",
    "def viterbi(sentence): #sentence eg ['START','Plato','degustación',':','un','poco','abundante','de','más',',','pero','bien','cocinado','.','STOP']\n",
    "    viterbi_matrix = pd.DataFrame(index=labels, columns=sentence)\n",
    "    best_back_path_matrix = pd.DataFrame(index=labels, columns=sentence)\n",
    "\n",
    "    #Init START emitting START to 1\n",
    "    #viterbi_matrix[label][word] represents the highest score of path from START to this node\n",
    "    viterbi_matrix.loc[:, 'START'] = 0\n",
    "    viterbi_matrix.loc['START', 'START'] = 1\n",
    "    best_back_path_matrix.loc[:, 'START'] = None\n",
    "    best_back_path_matrix.iloc[:, 1] = 'START'\n",
    "\n",
    "    #DP algorithm\n",
    "    for word_index in range(1,len(sentence)): #Start from second element, skip START\n",
    "        for label in labels:\n",
    "            max_score = 0\n",
    "            for prev_label in labels:\n",
    "                score = viterbi_matrix.iloc[viterbi_matrix.index.get_loc(prev_label), word_index-1] * emission_matrix[label][sentence[word_index]] * transition_matrix[label][prev_label]\n",
    "                if score > max_score:\n",
    "                    max_score = score\n",
    "                    best_back_path_matrix.iloc[viterbi_matrix.index.get_loc(label), word_index] = prev_label\n",
    "            viterbi_matrix.iloc[viterbi_matrix.index.get_loc(label), word_index] = max_score\n",
    "            if max_score == 0: #No chance for this label, ignore possible path\n",
    "                best_back_path_matrix.iloc[best_back_path_matrix.index.get_loc(label), word_index] = None\n",
    "\n",
    "    #From highest scoring value at STOP, find the best back path.\n",
    "    pointer = 'STOP'\n",
    "    best_path = []\n",
    "    for word_index in range(len(sentence)-1,-1,-1):\n",
    "        best_path.append(pointer)\n",
    "        pointer = best_back_path_matrix.iloc[best_back_path_matrix.index.get_loc(pointer), word_index]\n",
    "    best_path = best_path[::-1] #Reverse the array\n",
    "    best_path = best_path[1:-1] #Remove START and STOP\n",
    "    \n",
    "    return best_path\n",
    "\n",
    "#Helper function to replace all new words with #UNK# and add START and STOP\n",
    "def add_startstop_replace_new(sentence):\n",
    "    for index, word in enumerate(sentence):\n",
    "        if (df['Data'] == word).sum() == 0:\n",
    "            sentence[index] = '#UNK#'\n",
    "    sentence.insert(0, 'START')\n",
    "    sentence.append('STOP')\n",
    "\n",
    "#Wrapper function, call this.\n",
    "def predict_sentence_sentiment(sentence):\n",
    "    sentence_copy = sentence.copy()\n",
    "    add_startstop_replace_new(sentence_copy)\n",
    "    return viterbi(sentence_copy)\n",
    "\n",
    "#sentence = ['Plato','degustación',':','un','poco','abundante','de','más',',','pero','bien','cocinado','.']\n",
    "#predict_sentence_sentiment(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 2\n",
    "#Predict sentiment of dev.in\n",
    "\n",
    "#Read the data\n",
    "file_path = f\"Data/{language}/dev.in\"\n",
    "labeled_data = []\n",
    "\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "#Read data into array of sentence arrays\n",
    "sentences = [[]]\n",
    "sentence_number = 0\n",
    "for line in lines:\n",
    "    line = line.strip()\n",
    "    if line:\n",
    "        sentences[sentence_number].append(line)\n",
    "    else:\n",
    "        sentences.append([])\n",
    "        sentence_number += 1\n",
    "\n",
    "sentences_with_prediction = []\n",
    "for sentence in sentences:\n",
    "    try:\n",
    "        prediction = predict_sentence_sentiment(sentence)\n",
    "        # Using list comprehension\n",
    "        sentence_with_prediction = [s1 + ' ' + s2 for s1, s2 in zip(sentence, prediction)]\n",
    "        sentences_with_prediction.append(sentence_with_prediction)\n",
    "    except:\n",
    "        print(\"Error at sentence: \")\n",
    "        print(sentence)\n",
    "\n",
    "# Name of the output file\n",
    "file_name = f\"Outputs/{language}/dev.p2.out\"\n",
    "\n",
    "# Open the file in write mode and write each element of the array as a new line\n",
    "with open(file_name, \"w\", encoding='utf-8') as file:\n",
    "    for sentence in sentences_with_prediction:\n",
    "        for line in sentence:\n",
    "            file.write(line + \"\\n\")\n",
    "        file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['B-negative', 'I-negative', 'O', 'O', 'O', 'O', 'O', 'O', 'O'],\n",
       " ['B-negative', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'],\n",
       " ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'],\n",
       " ['B-positive', 'I-positive', 'O', 'O', 'O', 'O', 'O', 'O', 'O'],\n",
       " ['B-positive', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'],\n",
       " ['B-negative', 'I-negative', 'I-negative', 'O', 'O', 'O', 'O', 'O', 'O'],\n",
       " ['B-negative', 'I-negative', 'O', 'O', 'O', 'B-positive', 'O', 'O', 'O'],\n",
       " ['B-neutral', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Part 3\n",
    "#Viterbi Algorithm modified - find k-th best path\n",
    "\n",
    "def viterbi_multiple(sentence, k): #sentence eg ['START','Plato','degustación',':','un','poco','abundante','de','más',',','pero','bien','cocinado','.','STOP']\n",
    "    viterbi_matrix = np.zeros((len(labels), len(sentence), k)) \n",
    "    best_back_path_matrix = np.empty((len(labels), len(sentence), k), dtype=object) #best_bath_path_matrix[0,1,2] = (2,1) means third best score to first label, second word points back to second label, second best score\n",
    "\n",
    "    START_LABEL_INDEX = np.where(labels == 'START')[0][0]\n",
    "    STOP_LABEL_INDEX = np.where(labels == 'STOP')[0][0]\n",
    "\n",
    "    #Init START emitting START to 1\n",
    "    #viterbi_matrix[0,1,2] = 0.001 means third best score to first label, second word\n",
    "    viterbi_matrix[START_LABEL_INDEX, 0, 0] = 1 #Init (START, START, path 0) to 1\n",
    "    best_back_path_matrix[:, :, :] = None\n",
    "\n",
    "    #DP algorithm\n",
    "    for word_index in range(1,len(sentence)): #Start from second element, skip START\n",
    "        for label_index, label in enumerate(labels):\n",
    "            for prev_label_index, prev_label in enumerate(labels):\n",
    "                for depth in range(k): #Iterate over all k best paths from previous node\n",
    "                    score = viterbi_matrix[prev_label_index, word_index-1, depth] * emission_matrix[label][sentence[word_index]] * transition_matrix[label][prev_label]\n",
    "   \n",
    "                    #Find the position to insert the new value to maintain descending order\n",
    "                    #Use negation as searchsorted only works for ascending\n",
    "                    position = np.searchsorted(-viterbi_matrix[label_index, word_index, :], -score, side='right')\n",
    "\n",
    "                    #If position == k, the score is smaller than existing scores\n",
    "                    #Shift all values in both matrices from 'position' by one, then insert new element\n",
    "                    if position < k:\n",
    "                        for pointer in range(k-1, position, -1):\n",
    "                            viterbi_matrix[label_index, word_index, pointer] = viterbi_matrix[label_index, word_index, pointer-1]\n",
    "                            best_back_path_matrix[label_index, word_index, pointer] = best_back_path_matrix[label_index, word_index, pointer-1]\n",
    "                        viterbi_matrix[label_index, word_index, position] = score\n",
    "                        best_back_path_matrix[label_index, word_index, position] = (prev_label_index, depth)\n",
    "\n",
    "    #final_scores = viterbi_matrix[STOP_LABEL_INDEX, len(sentence)-1, :]\n",
    "    #print(final_scores)\n",
    "\n",
    "    #From all the highest scoring paths at STOP, find the best back path.\n",
    "    best_paths = []\n",
    "    for path_number in range(k):\n",
    "        pointer = STOP_LABEL_INDEX\n",
    "        depth_pointer = path_number\n",
    "        best_path = []\n",
    "        for word_index in range(len(sentence)-1,-1,-1):\n",
    "            best_path.append(labels[pointer])\n",
    "            if word_index > 0:\n",
    "                back_path = best_back_path_matrix[pointer, word_index, depth_pointer]\n",
    "                pointer = back_path[0]\n",
    "                depth_pointer = back_path[1]\n",
    "        best_path = best_path[::-1] #Reverse the array\n",
    "        best_path = best_path[1:-1] #Remove START and STOP\n",
    "        best_paths.append(best_path)\n",
    "    \n",
    "    return best_paths\n",
    "\n",
    "#Wrapper function, call this.\n",
    "def predict_sentence_sentiment_multiple(sentence, k):\n",
    "    sentence_copy = sentence.copy()\n",
    "    add_startstop_replace_new(sentence_copy) #Same as qn2\n",
    "    return viterbi_multiple(sentence_copy, k)\n",
    "\n",
    "sentence = ['Plato','degustación',':','un','poco','abundante','de','más','.']\n",
    "predict_sentence_sentiment_multiple(sentence, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
